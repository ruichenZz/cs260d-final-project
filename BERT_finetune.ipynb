{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 220\n",
    "SEED = 1234\n",
    "EPOCHS = 1\n",
    "Data_dir=\"./jigsaw-dataset\"\n",
    "WORK_DIR = \"../working/\"\n",
    "num_to_load=1000000                         #Train size to match time limit\n",
    "valid_size= 100000                          #Validation Size\n",
    "TOXICITY_COLUMN = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for text in tqdm_notebook(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    print(longer)\n",
    "    return np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Training set size: 144389\n",
      "Subset Validation set size: 36098\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "train = pd.read_csv(os.path.join(Data_dir, \"train.csv\"))\n",
    "\n",
    "# Clean the text data\n",
    "train['comment_text'] = train['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n', ' ', regex=True)\n",
    "\n",
    "# Add class labels (binary: 0 for non-toxic, 1 for toxic)\n",
    "train['label'] = np.where(train['target'] >= 0.5, 1, 0)\n",
    "\n",
    "# Retain only necessary columns\n",
    "train = train[['id', 'comment_text', 'label']]\n",
    "\n",
    "# Sample a 10% subset for demonstration\n",
    "train_subset = train.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Split into training and validation sets (80/20 split)\n",
    "train_data, val_data = train_test_split(train_subset, test_size=0.2, stratify=train_subset['label'], random_state=42)\n",
    "\n",
    "# Print data stats\n",
    "print(f\"Subset Training set size: {len(train_data)}\")\n",
    "print(f\"Subset Validation set size: {len(val_data)}\")\n",
    "\n",
    "# Save the sampled training and validation sets\n",
    "train_data.to_csv(\"train_subset.csv\", index=False)\n",
    "val_data.to_csv(\"val_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values after cleaning:\n",
      "Train set: 0\n",
      "Validation set: 0\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with null comment_text\n",
    "train_data = train_data.dropna(subset=['comment_text'])\n",
    "val_data = val_data.dropna(subset=['comment_text'])\n",
    "\n",
    "# Ensure all values are strings\n",
    "train_data['comment_text'] = train_data['comment_text'].astype(str)\n",
    "val_data['comment_text'] = val_data['comment_text'].astype(str)\n",
    "\n",
    "# Check data again\n",
    "print(\"Number of null values after cleaning:\")\n",
    "print(\"Train set:\", train_data['comment_text'].isnull().sum())\n",
    "print(\"Validation set:\", val_data['comment_text'].isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize data function\n",
    "def tokenize_data(data, text_column, label_column):\n",
    "    encodings = tokenizer(\n",
    "        list(data[text_column]),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = data[label_column].values\n",
    "    return encodings, labels\n",
    "\n",
    "# Tokenize training and validation data\n",
    "train_encodings, train_labels = tokenize_data(train_data, \"comment_text\", \"label\")\n",
    "val_encodings, val_labels = tokenize_data(val_data, \"comment_text\", \"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ToxicCommentsDataset(Dataset):\n",
    "    def __init__(self, ids, encodings, labels):\n",
    "        self.ids = ids  # Store ids for reference\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Only return id for reference, not for the model\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])  # Add labels for the model\n",
    "        item['id'] = self.ids[idx]  # Keep 'id' for external use only\n",
    "        return item\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ToxicCommentsDataset(train_data['id'].values, train_encodings, train_labels)\n",
    "val_dataset = ToxicCommentsDataset(val_data['id'].values, val_encodings, val_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/toxic/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 0/9025 [00:00<?, ?it/s]/var/tmp/ipykernel_13783/4115423148.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9025/9025 [51:58<00:00,  2.89it/s, loss=0.811]  \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "num_training_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    loop = tqdm(train_loader, desc=\"Training\", leave=True)\n",
    "    for batch in loop:\n",
    "        # Move inputs to the device\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != \"id\"}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Update tqdm with the current loss\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2257 [00:00<?, ?it/s]/var/tmp/ipykernel_13783/4115423148.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Evaluating: 100%|██████████| 2257/2257 [04:38<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Toxic       0.96      0.98      0.97     33207\n",
      "       Toxic       0.73      0.55      0.63      2891\n",
      "\n",
      "    accuracy                           0.95     36098\n",
      "   macro avg       0.85      0.77      0.80     36098\n",
      "weighted avg       0.94      0.95      0.94     36098\n",
      "\n",
      "Validation predictions saved to 'val_with_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "val_predictions, val_true_labels, val_ids = [], [], []  # Include val_ids to track original IDs\n",
    "\n",
    "with torch.no_grad():\n",
    "    loop = tqdm(val_loader, desc=\"Evaluating\", leave=True)\n",
    "    for batch in loop:\n",
    "        # Move inputs to the device, exclude 'id'\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch.items() if k != \"id\"}\n",
    "        outputs = model(**batch_inputs)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Collect predictions, true labels, and IDs\n",
    "        val_predictions.extend(preds.cpu().numpy())\n",
    "        val_true_labels.extend(batch['labels'].cpu().numpy())\n",
    "        val_ids.extend(batch['id'].cpu().numpy())  # Track the original IDs\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(val_true_labels, val_predictions, target_names=[\"Non-Toxic\", \"Toxic\"]))\n",
    "\n",
    "# Add predictions and true labels to the validation DataFrame for analysis\n",
    "val_data['id'] = val_ids\n",
    "val_data['true_label'] = val_true_labels\n",
    "val_data['predicted_label'] = val_predictions\n",
    "val_data['predicted_label'] = val_data['predicted_label'].map({0: \"non-toxic\", 1: \"toxic\"})\n",
    "\n",
    "# Save validation results\n",
    "val_data.to_csv(\"val_with_predictions.csv\", index=False)\n",
    "print(\"Validation predictions saved to 'val_with_predictions.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original dataset\n",
    "original_data = pd.read_csv(os.path.join(Data_dir, \"train.csv\"))\n",
    "\n",
    "# Load validation predictions\n",
    "val_predictions = pd.read_csv(\"val_with_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge validation predictions with the original dataset\n",
    "merged_data = val_predictions.merge(original_data, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subgroup columns\n",
    "subgroups = [\n",
    "    \"asian\", \"atheist\", \"bisexual\", \"black\", \"buddhist\", \"christian\",\n",
    "    \"female\", \"heterosexual\", \"hindu\", \"homosexual_gay_or_lesbian\",\n",
    "    \"intellectual_or_learning_disability\", \"jewish\", \"latino\", \"male\",\n",
    "    \"muslim\", \"other_disability\", \"other_gender\", \"other_race_or_ethnicity\",\n",
    "    \"other_religion\", \"other_sexual_orientation\", \"physical_disability\",\n",
    "    \"psychiatric_or_mental_illness\", \"transgender\", \"white\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Subgroup  Accuracy  Support\n",
      "0                                 asian       0.0       19\n",
      "1                               atheist       0.0        6\n",
      "2                              bisexual       0.0        1\n",
      "3                                 black       0.0      171\n",
      "4                              buddhist       0.0        1\n",
      "5                             christian       0.0      335\n",
      "6                                female       0.0      593\n",
      "7                          heterosexual       0.0        7\n",
      "8                                 hindu       0.0        3\n",
      "9             homosexual_gay_or_lesbian       0.0      100\n",
      "11                               jewish       0.0       74\n",
      "12                               latino       0.0       15\n",
      "13                                 male       0.0      413\n",
      "14                               muslim       0.0      266\n",
      "18                       other_religion       0.0        2\n",
      "21        psychiatric_or_mental_illness       0.0       35\n",
      "22                          transgender       0.0       17\n",
      "23                                white       0.0      268\n",
      "10  intellectual_or_learning_disability       NaN        0\n",
      "15                     other_disability       NaN        0\n",
      "16                         other_gender       NaN        0\n",
      "17              other_race_or_ethnicity       NaN        0\n",
      "19             other_sexual_orientation       NaN        0\n",
      "20                  physical_disability       NaN        0\n",
      "Subgroup accuracies saved to 'subgroup_accuracies.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionary to store results\n",
    "subgroup_accuracies = {}\n",
    "\n",
    "# Calculate accuracy for each subgroup\n",
    "for subgroup in subgroups:\n",
    "    # Filter rows where the subgroup column is 1 (indicating membership in the subgroup)\n",
    "    subgroup_data = merged_data[merged_data[subgroup] == 1]\n",
    "    \n",
    "    if not subgroup_data.empty:\n",
    "        # Calculate accuracy: (correct predictions / total predictions for the subgroup)\n",
    "        correct_predictions = (subgroup_data['true_label'] == subgroup_data['predicted_label']).sum()\n",
    "        total_predictions = len(subgroup_data)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        subgroup_accuracies[subgroup] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"support\": total_predictions\n",
    "        }\n",
    "    else:\n",
    "        # Handle cases where there are no members in the subgroup\n",
    "        subgroup_accuracies[subgroup] = {\n",
    "            \"accuracy\": None,\n",
    "            \"support\": 0\n",
    "        }\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "subgroup_accuracies_df = pd.DataFrame.from_dict(subgroup_accuracies, orient=\"index\")\n",
    "subgroup_accuracies_df.reset_index(inplace=True)\n",
    "subgroup_accuracies_df.columns = [\"Subgroup\", \"Accuracy\", \"Support\"]\n",
    "\n",
    "# Sort the DataFrame by accuracy\n",
    "subgroup_accuracies_df = subgroup_accuracies_df.sort_values(by=\"Accuracy\", ascending=False)\n",
    "\n",
    "# Display the results\n",
    "print(subgroup_accuracies_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "subgroup_accuracies_df.to_csv(\"subgroup_accuracies.csv\", index=False)\n",
    "print(\"Subgroup accuracies saved to 'subgroup_accuracies.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in true_label: [0 1]\n",
      "Unique values in predicted_label: ['non-toxic' 'toxic']\n"
     ]
    }
   ],
   "source": [
    "# Check the unique values of true_label and predicted_label\n",
    "print(\"Unique values in true_label:\", merged_data['true_label'].unique())\n",
    "print(\"Unique values in predicted_label:\", merged_data['predicted_label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure labels are numeric\n",
    "merged_data['true_label'] = merged_data['true_label'].map({\"non-toxic\": 0, \"toxic\": 1})\n",
    "merged_data['predicted_label'] = merged_data['predicted_label'].map({\"non-toxic\": 0, \"toxic\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asian                                    80.784453\n",
      "atheist                                  19.800560\n",
      "bisexual                                 14.468329\n",
      "black                                   300.233333\n",
      "buddhist                                  9.066667\n",
      "christian                               766.314440\n",
      "female                                 1047.201103\n",
      "heterosexual                             29.401111\n",
      "hindu                                    13.383893\n",
      "homosexual_gay_or_lesbian               212.602765\n",
      "intellectual_or_learning_disability       8.571671\n",
      "jewish                                  143.715631\n",
      "latino                                   51.514194\n",
      "male                                    939.515413\n",
      "muslim                                  407.021212\n",
      "other_disability                          8.553876\n",
      "other_gender                              6.668338\n",
      "other_race_or_ethnicity                  71.263584\n",
      "other_religion                           57.530384\n",
      "other_sexual_orientation                 14.253756\n",
      "physical_disability                       9.151111\n",
      "psychiatric_or_mental_illness           117.287072\n",
      "transgender                              55.564847\n",
      "white                                   465.922152\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check the counts of members in each subgroup\n",
    "subgroup_counts = merged_data[subgroups].sum()\n",
    "print(subgroup_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in 'asian' subgroup: 19\n",
      "Correctly predicted samples: 0\n"
     ]
    }
   ],
   "source": [
    "# Debug accuracy for the 'asian' subgroup\n",
    "asian_data = merged_data[merged_data['asian'] == 1]\n",
    "print(\"Number of samples in 'asian' subgroup:\", len(asian_data))\n",
    "print(\"Correctly predicted samples:\", (asian_data['true_label'] == asian_data['predicted_label']).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Subgroup  Accuracy  Support\n",
      "0                                 asian  0.789474       19\n",
      "1                               atheist  0.833333        6\n",
      "2                              bisexual  1.000000        1\n",
      "3                                 black  0.725146      171\n",
      "4                              buddhist  1.000000        1\n",
      "5                             christian  0.901493      335\n",
      "6                                female  0.917369      593\n",
      "7                          heterosexual  0.857143        7\n",
      "8                                 hindu  1.000000        3\n",
      "9             homosexual_gay_or_lesbian  0.770000      100\n",
      "10  intellectual_or_learning_disability       NaN        0\n",
      "11                               jewish  0.878378       74\n",
      "12                               latino  0.933333       15\n",
      "13                                 male  0.907990      413\n",
      "14                               muslim  0.830827      266\n",
      "15                     other_disability       NaN        0\n",
      "16                         other_gender       NaN        0\n",
      "17              other_race_or_ethnicity       NaN        0\n",
      "18                       other_religion  1.000000        2\n",
      "19             other_sexual_orientation       NaN        0\n",
      "20                  physical_disability       NaN        0\n",
      "21        psychiatric_or_mental_illness  0.885714       35\n",
      "22                          transgender  0.823529       17\n",
      "23                                white  0.802239      268\n"
     ]
    }
   ],
   "source": [
    "# Recalculate subgroup accuracies\n",
    "subgroup_accuracies = {}\n",
    "\n",
    "for subgroup in subgroups:\n",
    "    subgroup_data = merged_data[merged_data[subgroup] == 1]\n",
    "    if not subgroup_data.empty:\n",
    "        correct_predictions = (subgroup_data['label'] == subgroup_data['predicted_label']).sum()\n",
    "        total_predictions = len(subgroup_data)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        subgroup_accuracies[subgroup] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"support\": total_predictions\n",
    "        }\n",
    "    else:\n",
    "        subgroup_accuracies[subgroup] = {\n",
    "            \"accuracy\": None,\n",
    "            \"support\": 0\n",
    "        }\n",
    "\n",
    "# Convert to DataFrame\n",
    "subgroup_accuracies_df = pd.DataFrame.from_dict(subgroup_accuracies, orient=\"index\")\n",
    "subgroup_accuracies_df.reset_index(inplace=True)\n",
    "subgroup_accuracies_df.columns = [\"Subgroup\", \"Accuracy\", \"Support\"]\n",
    "print(subgroup_accuracies_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Subgroup       FPR  Support\n",
      "7                          heterosexual  0.166667        6\n",
      "3                                 black  0.121495      107\n",
      "22                          transgender  0.066667       15\n",
      "9             homosexual_gay_or_lesbian  0.054054       74\n",
      "23                                white  0.042105      190\n",
      "21        psychiatric_or_mental_illness  0.035714       28\n",
      "13                                 male  0.031977      344\n",
      "5                             christian  0.026936      297\n",
      "14                               muslim  0.019608      204\n",
      "6                                female  0.017308      520\n",
      "1                               atheist  0.000000        5\n",
      "0                                 asian  0.000000       15\n",
      "2                              bisexual  0.000000        1\n",
      "4                              buddhist  0.000000        1\n",
      "8                                 hindu  0.000000        3\n",
      "12                               latino  0.000000       13\n",
      "11                               jewish  0.000000       61\n",
      "18                       other_religion  0.000000        2\n",
      "10  intellectual_or_learning_disability       NaN        0\n",
      "15                     other_disability       NaN        0\n",
      "16                         other_gender       NaN        0\n",
      "17              other_race_or_ethnicity       NaN        0\n",
      "19             other_sexual_orientation       NaN        0\n",
      "20                  physical_disability       NaN        0\n",
      "Subgroup FPR saved to 'subgroup_fpr.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store subgroup FPRs\n",
    "subgroup_fpr = {}\n",
    "\n",
    "for subgroup in subgroups:\n",
    "    # Filter rows where the subgroup column is 1\n",
    "    subgroup_data = merged_data[merged_data[subgroup] == 1]\n",
    "    \n",
    "    if not subgroup_data.empty:\n",
    "        # Calculate False Positives (FP) and True Negatives (TN)\n",
    "        fp = ((subgroup_data['label'] == 0) & (subgroup_data['predicted_label'] == 1)).sum()\n",
    "        tn = ((subgroup_data['label'] == 0) & (subgroup_data['predicted_label'] == 0)).sum()\n",
    "        \n",
    "        # Calculate FPR: FP / (FP + TN)\n",
    "        actual_negatives = fp + tn\n",
    "        fpr = fp / actual_negatives if actual_negatives > 0 else None\n",
    "        subgroup_fpr[subgroup] = {\n",
    "            \"FPR\": fpr,\n",
    "            \"Support\": actual_negatives\n",
    "        }\n",
    "    else:\n",
    "        # Handle cases where there are no members of the subgroup\n",
    "        subgroup_fpr[subgroup] = {\n",
    "            \"FPR\": None,\n",
    "            \"Support\": 0\n",
    "        }\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "subgroup_fpr_df = pd.DataFrame.from_dict(subgroup_fpr, orient=\"index\")\n",
    "subgroup_fpr_df.reset_index(inplace=True)\n",
    "subgroup_fpr_df.columns = [\"Subgroup\", \"FPR\", \"Support\"]\n",
    "\n",
    "# Sort by FPR\n",
    "subgroup_fpr_df = subgroup_fpr_df.sort_values(by=\"FPR\", ascending=False)\n",
    "\n",
    "# Display the results\n",
    "print(subgroup_fpr_df)\n",
    "\n",
    "# Save results to a CSV\n",
    "subgroup_fpr_df.to_csv(\"subgroup_fpr.csv\", index=False)\n",
    "print(\"Subgroup FPR saved to 'subgroup_fpr.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Plot subgroup FPRs\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot subgroup FPRs\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(subgroup_fpr_df['Subgroup'], subgroup_fpr_df['FPR'], color='lightcoral')\n",
    "plt.xlabel('Subgroup', fontsize=14)\n",
    "plt.ylabel('False Positive Rate (FPR)', fontsize=14)\n",
    "plt.title('Subgroup False Positive Rates', fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to 'bert_model_checkpoint'\n"
     ]
    }
   ],
   "source": [
    "# Directory to save the model\n",
    "output_dir = \"bert_model_checkpoint\"\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved to '{output_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
